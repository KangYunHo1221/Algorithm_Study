{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "import math\n",
    "from torch import nn, optim, cuda\n",
    "from torch.utils import data\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "device = 'cuda' if cuda.is_available() else 'cpu'\n",
    "train_dataset = datasets.MNIST(root='./data/',\n",
    "                               train=True,\n",
    "                               transform=transforms.ToTensor(),\n",
    "                               download=True)\n",
    "\n",
    "test_dataset = datasets.MNIST(root='./data/',\n",
    "                              train=False,\n",
    "                              transform=transforms.ToTensor())\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size,\n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size,\n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InceptionA(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(InceptionA, self).__init__()\n",
    "        self.branch1x1 = nn.Conv2d(in_channels, 16, kernel_size = 1)\n",
    "        \n",
    "        self.branch5x5_1 = nn.Conv2d(in_channels, 16, kernel_size=1)\n",
    "        self.branch5x5_2 = nn.Conv2d(16,24,kernel_size = 5, padding =2)\n",
    "        \n",
    "        self.branch3x3dbl_1 = nn.Conv2d(in_channels, 16, kernel_size = 1)\n",
    "        self.branch3x3dbl_2 = nn.Conv2d(16, 24, kernel_size = 3, padding =1)\n",
    "        self.branch3x3dbl_3 = nn.Conv2d(24, 24, kernel_size = 3, padding =1)\n",
    "        \n",
    "        self.branch_pool = nn.Conv2d(in_channels, 24, kernel_size = 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        branch1x1 = self.branch1x1(x)\n",
    "        \n",
    "        branch5x5 = self.branch5x5_1(x)\n",
    "        branch5x5 = self.branch5x5_2(branch5x5)\n",
    "        \n",
    "        branch3x3dbl = self.branch3x3dbl_1(x)\n",
    "        branch3x3dbl = self.branch3x3dbl_2(branch3x3dbl)\n",
    "        branch3x3dbl = self.branch3x3dbl_3(branch3x3dbl)\n",
    "        \n",
    "        branch_pool = F.avg_pool2d(x, kernel_size = 3, stride = 1, padding = 1)\n",
    "        branch_pool = self.branch_pool(branch_pool)\n",
    "        \n",
    "        outputs = [branch1x1, branch5x5, branch3x3dbl, branch_pool]\n",
    "        \n",
    "        return torch.cat(outputs, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size = 5)\n",
    "        self.conv2 = nn.Conv2d(88, 20, kernel_size = 5)\n",
    "        \n",
    "        self.incept1 = InceptionA(in_channels = 10)\n",
    "        self.incept2 = InceptionA(in_channels = 20)\n",
    "        \n",
    "        self.mp = nn.MaxPool2d(2)\n",
    "        self.fc = nn.Linear(1408, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        in_size = x.size(0)\n",
    "        x = F.relu(self.mp(self.conv1(x)))\n",
    "        x = self.incept1(x)\n",
    "        x = F.relu(self.mp(self.conv2(x)))\n",
    "        x = self.incept2(x)\n",
    "        x = x.view(in_size, -1)\n",
    "        x = self.fc(x)\n",
    "        return F.log_softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = net()\n",
    "model.to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr = 0.01, momentum = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, target in test_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        output = model(data)\n",
    "        # sum up batch loss()\n",
    "        test_loss += F.nll_loss(output, target, size_average=False).item()\n",
    "        # get the index of the max log-probability\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-ea14773f994e>:21: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.312578\n",
      "Train Epoch: 1 [640/60000 (1%)]\tLoss: 2.304968\n",
      "Train Epoch: 1 [1280/60000 (2%)]\tLoss: 2.300013\n",
      "Train Epoch: 1 [1920/60000 (3%)]\tLoss: 2.301765\n",
      "Train Epoch: 1 [2560/60000 (4%)]\tLoss: 2.286862\n",
      "Train Epoch: 1 [3200/60000 (5%)]\tLoss: 2.297586\n",
      "Train Epoch: 1 [3840/60000 (6%)]\tLoss: 2.287486\n",
      "Train Epoch: 1 [4480/60000 (7%)]\tLoss: 2.303152\n",
      "Train Epoch: 1 [5120/60000 (9%)]\tLoss: 2.286756\n",
      "Train Epoch: 1 [5760/60000 (10%)]\tLoss: 2.285491\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 2.267778\n",
      "Train Epoch: 1 [7040/60000 (12%)]\tLoss: 2.257797\n",
      "Train Epoch: 1 [7680/60000 (13%)]\tLoss: 2.251420\n",
      "Train Epoch: 1 [8320/60000 (14%)]\tLoss: 2.240700\n",
      "Train Epoch: 1 [8960/60000 (15%)]\tLoss: 2.211678\n",
      "Train Epoch: 1 [9600/60000 (16%)]\tLoss: 2.169512\n",
      "Train Epoch: 1 [10240/60000 (17%)]\tLoss: 2.111501\n",
      "Train Epoch: 1 [10880/60000 (18%)]\tLoss: 1.972783\n",
      "Train Epoch: 1 [11520/60000 (19%)]\tLoss: 1.798708\n",
      "Train Epoch: 1 [12160/60000 (20%)]\tLoss: 1.464570\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 1.149430\n",
      "Train Epoch: 1 [13440/60000 (22%)]\tLoss: 0.734911\n",
      "Train Epoch: 1 [14080/60000 (23%)]\tLoss: 0.790012\n",
      "Train Epoch: 1 [14720/60000 (25%)]\tLoss: 0.784238\n",
      "Train Epoch: 1 [15360/60000 (26%)]\tLoss: 0.702843\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.467236\n",
      "Train Epoch: 1 [16640/60000 (28%)]\tLoss: 0.667047\n",
      "Train Epoch: 1 [17280/60000 (29%)]\tLoss: 0.418174\n",
      "Train Epoch: 1 [17920/60000 (30%)]\tLoss: 0.599883\n",
      "Train Epoch: 1 [18560/60000 (31%)]\tLoss: 0.533239\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.670027\n",
      "Train Epoch: 1 [19840/60000 (33%)]\tLoss: 0.401809\n",
      "Train Epoch: 1 [20480/60000 (34%)]\tLoss: 0.585274\n",
      "Train Epoch: 1 [21120/60000 (35%)]\tLoss: 0.478692\n",
      "Train Epoch: 1 [21760/60000 (36%)]\tLoss: 0.448598\n",
      "Train Epoch: 1 [22400/60000 (37%)]\tLoss: 0.596099\n",
      "Train Epoch: 1 [23040/60000 (38%)]\tLoss: 0.450325\n",
      "Train Epoch: 1 [23680/60000 (39%)]\tLoss: 0.497247\n",
      "Train Epoch: 1 [24320/60000 (41%)]\tLoss: 0.424751\n",
      "Train Epoch: 1 [24960/60000 (42%)]\tLoss: 0.520236\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.430177\n",
      "Train Epoch: 1 [26240/60000 (44%)]\tLoss: 0.293960\n",
      "Train Epoch: 1 [26880/60000 (45%)]\tLoss: 0.462344\n",
      "Train Epoch: 1 [27520/60000 (46%)]\tLoss: 0.526886\n",
      "Train Epoch: 1 [28160/60000 (47%)]\tLoss: 0.304458\n",
      "Train Epoch: 1 [28800/60000 (48%)]\tLoss: 0.382346\n",
      "Train Epoch: 1 [29440/60000 (49%)]\tLoss: 0.304864\n",
      "Train Epoch: 1 [30080/60000 (50%)]\tLoss: 0.206792\n",
      "Train Epoch: 1 [30720/60000 (51%)]\tLoss: 0.251039\n",
      "Train Epoch: 1 [31360/60000 (52%)]\tLoss: 0.239275\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.273754\n",
      "Train Epoch: 1 [32640/60000 (54%)]\tLoss: 0.215724\n",
      "Train Epoch: 1 [33280/60000 (55%)]\tLoss: 0.505583\n",
      "Train Epoch: 1 [33920/60000 (57%)]\tLoss: 0.365269\n",
      "Train Epoch: 1 [34560/60000 (58%)]\tLoss: 0.488730\n",
      "Train Epoch: 1 [35200/60000 (59%)]\tLoss: 0.278891\n",
      "Train Epoch: 1 [35840/60000 (60%)]\tLoss: 0.421582\n",
      "Train Epoch: 1 [36480/60000 (61%)]\tLoss: 0.374788\n",
      "Train Epoch: 1 [37120/60000 (62%)]\tLoss: 0.323736\n",
      "Train Epoch: 1 [37760/60000 (63%)]\tLoss: 0.280153\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.475307\n",
      "Train Epoch: 1 [39040/60000 (65%)]\tLoss: 0.253213\n",
      "Train Epoch: 1 [39680/60000 (66%)]\tLoss: 0.253561\n",
      "Train Epoch: 1 [40320/60000 (67%)]\tLoss: 0.227720\n",
      "Train Epoch: 1 [40960/60000 (68%)]\tLoss: 0.266940\n",
      "Train Epoch: 1 [41600/60000 (69%)]\tLoss: 0.191631\n",
      "Train Epoch: 1 [42240/60000 (70%)]\tLoss: 0.221564\n",
      "Train Epoch: 1 [42880/60000 (71%)]\tLoss: 0.290033\n",
      "Train Epoch: 1 [43520/60000 (72%)]\tLoss: 0.364670\n",
      "Train Epoch: 1 [44160/60000 (74%)]\tLoss: 0.260132\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.455401\n",
      "Train Epoch: 1 [45440/60000 (76%)]\tLoss: 0.339763\n",
      "Train Epoch: 1 [46080/60000 (77%)]\tLoss: 0.257492\n",
      "Train Epoch: 1 [46720/60000 (78%)]\tLoss: 0.295782\n",
      "Train Epoch: 1 [47360/60000 (79%)]\tLoss: 0.443798\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.179764\n",
      "Train Epoch: 1 [48640/60000 (81%)]\tLoss: 0.227907\n",
      "Train Epoch: 1 [49280/60000 (82%)]\tLoss: 0.369498\n",
      "Train Epoch: 1 [49920/60000 (83%)]\tLoss: 0.464795\n",
      "Train Epoch: 1 [50560/60000 (84%)]\tLoss: 0.138678\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.287336\n",
      "Train Epoch: 1 [51840/60000 (86%)]\tLoss: 0.313817\n",
      "Train Epoch: 1 [52480/60000 (87%)]\tLoss: 0.242583\n",
      "Train Epoch: 1 [53120/60000 (88%)]\tLoss: 0.187770\n",
      "Train Epoch: 1 [53760/60000 (90%)]\tLoss: 0.233236\n",
      "Train Epoch: 1 [54400/60000 (91%)]\tLoss: 0.233876\n",
      "Train Epoch: 1 [55040/60000 (92%)]\tLoss: 0.472130\n",
      "Train Epoch: 1 [55680/60000 (93%)]\tLoss: 0.441114\n",
      "Train Epoch: 1 [56320/60000 (94%)]\tLoss: 0.140956\n",
      "Train Epoch: 1 [56960/60000 (95%)]\tLoss: 0.223450\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.217606\n",
      "Train Epoch: 1 [58240/60000 (97%)]\tLoss: 0.277258\n",
      "Train Epoch: 1 [58880/60000 (98%)]\tLoss: 0.171219\n",
      "Train Epoch: 1 [59520/60000 (99%)]\tLoss: 0.089699\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ansgh\\anaconda3\\envs\\pytorch17_p38\\lib\\site-packages\\torch\\nn\\_reduction.py:44: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.2371, Accuracy: 9254/10000 (93%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.454623\n",
      "Train Epoch: 2 [640/60000 (1%)]\tLoss: 0.233276\n",
      "Train Epoch: 2 [1280/60000 (2%)]\tLoss: 0.143055\n",
      "Train Epoch: 2 [1920/60000 (3%)]\tLoss: 0.251886\n",
      "Train Epoch: 2 [2560/60000 (4%)]\tLoss: 0.219599\n",
      "Train Epoch: 2 [3200/60000 (5%)]\tLoss: 0.167889\n",
      "Train Epoch: 2 [3840/60000 (6%)]\tLoss: 0.393366\n",
      "Train Epoch: 2 [4480/60000 (7%)]\tLoss: 0.328309\n",
      "Train Epoch: 2 [5120/60000 (9%)]\tLoss: 0.141692\n",
      "Train Epoch: 2 [5760/60000 (10%)]\tLoss: 0.206198\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.134557\n",
      "Train Epoch: 2 [7040/60000 (12%)]\tLoss: 0.166322\n",
      "Train Epoch: 2 [7680/60000 (13%)]\tLoss: 0.256308\n",
      "Train Epoch: 2 [8320/60000 (14%)]\tLoss: 0.206316\n",
      "Train Epoch: 2 [8960/60000 (15%)]\tLoss: 0.099062\n",
      "Train Epoch: 2 [9600/60000 (16%)]\tLoss: 0.164992\n",
      "Train Epoch: 2 [10240/60000 (17%)]\tLoss: 0.395912\n",
      "Train Epoch: 2 [10880/60000 (18%)]\tLoss: 0.132481\n",
      "Train Epoch: 2 [11520/60000 (19%)]\tLoss: 0.276699\n",
      "Train Epoch: 2 [12160/60000 (20%)]\tLoss: 0.046591\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.418848\n",
      "Train Epoch: 2 [13440/60000 (22%)]\tLoss: 0.173761\n",
      "Train Epoch: 2 [14080/60000 (23%)]\tLoss: 0.223590\n",
      "Train Epoch: 2 [14720/60000 (25%)]\tLoss: 0.247958\n",
      "Train Epoch: 2 [15360/60000 (26%)]\tLoss: 0.183022\n",
      "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.200817\n",
      "Train Epoch: 2 [16640/60000 (28%)]\tLoss: 0.242724\n",
      "Train Epoch: 2 [17280/60000 (29%)]\tLoss: 0.372326\n",
      "Train Epoch: 2 [17920/60000 (30%)]\tLoss: 0.082847\n",
      "Train Epoch: 2 [18560/60000 (31%)]\tLoss: 0.268828\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.168522\n",
      "Train Epoch: 2 [19840/60000 (33%)]\tLoss: 0.085640\n",
      "Train Epoch: 2 [20480/60000 (34%)]\tLoss: 0.101035\n",
      "Train Epoch: 2 [21120/60000 (35%)]\tLoss: 0.222252\n",
      "Train Epoch: 2 [21760/60000 (36%)]\tLoss: 0.156226\n",
      "Train Epoch: 2 [22400/60000 (37%)]\tLoss: 0.063601\n",
      "Train Epoch: 2 [23040/60000 (38%)]\tLoss: 0.184302\n",
      "Train Epoch: 2 [23680/60000 (39%)]\tLoss: 0.401591\n",
      "Train Epoch: 2 [24320/60000 (41%)]\tLoss: 0.153458\n",
      "Train Epoch: 2 [24960/60000 (42%)]\tLoss: 0.213315\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.364749\n",
      "Train Epoch: 2 [26240/60000 (44%)]\tLoss: 0.392234\n",
      "Train Epoch: 2 [26880/60000 (45%)]\tLoss: 0.179122\n",
      "Train Epoch: 2 [27520/60000 (46%)]\tLoss: 0.175816\n",
      "Train Epoch: 2 [28160/60000 (47%)]\tLoss: 0.136329\n",
      "Train Epoch: 2 [28800/60000 (48%)]\tLoss: 0.157796\n",
      "Train Epoch: 2 [29440/60000 (49%)]\tLoss: 0.111106\n",
      "Train Epoch: 2 [30080/60000 (50%)]\tLoss: 0.195090\n",
      "Train Epoch: 2 [30720/60000 (51%)]\tLoss: 0.153378\n",
      "Train Epoch: 2 [31360/60000 (52%)]\tLoss: 0.121805\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.137066\n",
      "Train Epoch: 2 [32640/60000 (54%)]\tLoss: 0.051732\n",
      "Train Epoch: 2 [33280/60000 (55%)]\tLoss: 0.208469\n",
      "Train Epoch: 2 [33920/60000 (57%)]\tLoss: 0.181857\n",
      "Train Epoch: 2 [34560/60000 (58%)]\tLoss: 0.149762\n",
      "Train Epoch: 2 [35200/60000 (59%)]\tLoss: 0.162385\n",
      "Train Epoch: 2 [35840/60000 (60%)]\tLoss: 0.130764\n",
      "Train Epoch: 2 [36480/60000 (61%)]\tLoss: 0.086767\n",
      "Train Epoch: 2 [37120/60000 (62%)]\tLoss: 0.115451\n",
      "Train Epoch: 2 [37760/60000 (63%)]\tLoss: 0.295299\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.120913\n",
      "Train Epoch: 2 [39040/60000 (65%)]\tLoss: 0.256175\n",
      "Train Epoch: 2 [39680/60000 (66%)]\tLoss: 0.119545\n",
      "Train Epoch: 2 [40320/60000 (67%)]\tLoss: 0.311862\n",
      "Train Epoch: 2 [40960/60000 (68%)]\tLoss: 0.099789\n",
      "Train Epoch: 2 [41600/60000 (69%)]\tLoss: 0.073862\n",
      "Train Epoch: 2 [42240/60000 (70%)]\tLoss: 0.202427\n",
      "Train Epoch: 2 [42880/60000 (71%)]\tLoss: 0.401206\n",
      "Train Epoch: 2 [43520/60000 (72%)]\tLoss: 0.063327\n",
      "Train Epoch: 2 [44160/60000 (74%)]\tLoss: 0.163828\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.114048\n",
      "Train Epoch: 2 [45440/60000 (76%)]\tLoss: 0.072897\n",
      "Train Epoch: 2 [46080/60000 (77%)]\tLoss: 0.063059\n",
      "Train Epoch: 2 [46720/60000 (78%)]\tLoss: 0.184442\n",
      "Train Epoch: 2 [47360/60000 (79%)]\tLoss: 0.154479\n",
      "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.109112\n",
      "Train Epoch: 2 [48640/60000 (81%)]\tLoss: 0.175406\n",
      "Train Epoch: 2 [49280/60000 (82%)]\tLoss: 0.078552\n",
      "Train Epoch: 2 [49920/60000 (83%)]\tLoss: 0.218811\n",
      "Train Epoch: 2 [50560/60000 (84%)]\tLoss: 0.511934\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.169368\n",
      "Train Epoch: 2 [51840/60000 (86%)]\tLoss: 0.147588\n",
      "Train Epoch: 2 [52480/60000 (87%)]\tLoss: 0.101269\n",
      "Train Epoch: 2 [53120/60000 (88%)]\tLoss: 0.035206\n",
      "Train Epoch: 2 [53760/60000 (90%)]\tLoss: 0.102208\n",
      "Train Epoch: 2 [54400/60000 (91%)]\tLoss: 0.099642\n",
      "Train Epoch: 2 [55040/60000 (92%)]\tLoss: 0.164605\n",
      "Train Epoch: 2 [55680/60000 (93%)]\tLoss: 0.086948\n",
      "Train Epoch: 2 [56320/60000 (94%)]\tLoss: 0.182432\n",
      "Train Epoch: 2 [56960/60000 (95%)]\tLoss: 0.177870\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.071129\n",
      "Train Epoch: 2 [58240/60000 (97%)]\tLoss: 0.124464\n",
      "Train Epoch: 2 [58880/60000 (98%)]\tLoss: 0.184051\n",
      "Train Epoch: 2 [59520/60000 (99%)]\tLoss: 0.104933\n",
      "\n",
      "Test set: Average loss: 0.1166, Accuracy: 9643/10000 (96%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.075278\n",
      "Train Epoch: 3 [640/60000 (1%)]\tLoss: 0.212244\n",
      "Train Epoch: 3 [1280/60000 (2%)]\tLoss: 0.175890\n",
      "Train Epoch: 3 [1920/60000 (3%)]\tLoss: 0.023837\n",
      "Train Epoch: 3 [2560/60000 (4%)]\tLoss: 0.080664\n",
      "Train Epoch: 3 [3200/60000 (5%)]\tLoss: 0.084817\n",
      "Train Epoch: 3 [3840/60000 (6%)]\tLoss: 0.153292\n",
      "Train Epoch: 3 [4480/60000 (7%)]\tLoss: 0.165428\n",
      "Train Epoch: 3 [5120/60000 (9%)]\tLoss: 0.234933\n",
      "Train Epoch: 3 [5760/60000 (10%)]\tLoss: 0.119319\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.145213\n",
      "Train Epoch: 3 [7040/60000 (12%)]\tLoss: 0.061768\n",
      "Train Epoch: 3 [7680/60000 (13%)]\tLoss: 0.119512\n",
      "Train Epoch: 3 [8320/60000 (14%)]\tLoss: 0.125702\n",
      "Train Epoch: 3 [8960/60000 (15%)]\tLoss: 0.078505\n",
      "Train Epoch: 3 [9600/60000 (16%)]\tLoss: 0.193242\n",
      "Train Epoch: 3 [10240/60000 (17%)]\tLoss: 0.114148\n",
      "Train Epoch: 3 [10880/60000 (18%)]\tLoss: 0.028223\n",
      "Train Epoch: 3 [11520/60000 (19%)]\tLoss: 0.120107\n",
      "Train Epoch: 3 [12160/60000 (20%)]\tLoss: 0.051926\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.192038\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, 10):\n",
    "    train(epoch)\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch17_p38",
   "language": "python",
   "name": "pytorch17_p38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
