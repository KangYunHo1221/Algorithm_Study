{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q pysndfx SoundFile audiomentations pretrainedmodels efficientnet_pytorch resnest","metadata":{"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport librosa as lb\nimport librosa.display as lbd\nimport soundfile as sf\nfrom  soundfile import SoundFile\nimport pandas as pd\nfrom  IPython.display import Audio\nfrom pathlib import Path\n\nimport torch\nfrom torch import nn, optim\nfrom  torch.utils.data import Dataset, DataLoader\nimport torch.nn.functional as F\n\nfrom matplotlib import pyplot as plt\n\nimport os, random, gc\nimport re, time, json\nfrom  ast import literal_eval\n\n\nfrom IPython.display import Audio\nfrom sklearn.metrics import label_ranking_average_precision_score\n\nfrom tqdm.notebook import tqdm\nimport joblib\n\nfrom resnest.torch import resnest50\nfrom efficientnet_pytorch import EfficientNet\nimport pretrainedmodels\nimport resnest.torch as resnest_torch","metadata":{"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"def seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\nseed_everything()\n\n\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"DATA_ROOT = Path(\"../input/birdclef-2021\")\nTRAIN_AUDIO_ROOT = Path(\"../input/birdclef-2021/train_short_audio\")\nTRAIN_AUDIO_IMAGES_ROOT = Path(\"../input/kkiller-birdclef-2021/audio_images\")\n\nNUM_CLASSES = 397\nSR = 32_000\nDURATION = 7\n\nMAX_READ_SAMPLES = 5 # Each record will have 10 melspecs at most, you can increase this on Colab with High Memory Enabled\n\n###\nMEL_PATHS = sorted(Path(\"../input\").glob(\"kkiller-birdclef-mels-computer-d7-part?/rich_train_metadata.csv\"))\nTRAIN_LABEL_PATHS = sorted(Path(\"../input\").glob(\"kkiller-birdclef-mels-computer-d7-part?/LABEL_IDS.json\"))\nMODEL_ROOT = Path(\".\")\n\nTRAIN_BATCH_SIZE = 50\nTRAIN_NUM_WORKERS = 2\n\nVAL_BATCH_SIZE = 50\nVAL_NUM_WORKERS = 2","metadata":{"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"##df 불러오기\n\ndef get_df(mel_paths=MEL_PATHS, train_label_paths=TRAIN_LABEL_PATHS):\n    df = None\n    LABEL_IDS = {}\n    \n    # mel_paths = rich_train_metadata\n    for file_path in mel_paths:\n        temp = pd.read_csv(str(file_path), index_col=0)\n        temp[\"impath\"] = temp.apply(lambda row: file_path.parent/\"audio_images/{}/{}.npy\".format(row.primary_label, row.filename), axis=1) \n        df = temp if df is None else df.append(temp)\n    \n    df[\"secondary_labels\"] = df[\"secondary_labels\"].apply(literal_eval)\n\n    for file_path in train_label_paths:\n        with open(str(file_path)) as f:\n            LABEL_IDS.update(json.load(f))\n        \n    return LABEL_IDS, df","metadata":{"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"LABEL_IDS, df = get_df()","metadata":{"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"##LABEL_IDS","metadata":{"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"##df.head()","metadata":{"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"## df.loc[df['rating'] < 2.5]","metadata":{"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"##이름순으로 총 387종류 있고, primary label에 최대 500회까지 있고 없는애는 8번밖에 없음\n## print(df[\"primary_label\"].value_counts())\n## print(df[\"label_id\"].min(), df[\"label_id\"].max())","metadata":{"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"## filename 이랑 impath랑 dic으로 매칭시\ndef load_data(df):\n    def load_row(row):\n        # impath = TRAIN_IMAGES_ROOT/f\"{row.primary_label}/{row.filename}.npy\"\n        # filename == @@@.ogg,     impath == .npy\n        return row.filename, np.load(str(row.impath))[:MAX_READ_SAMPLES]\n    \n    pool = joblib.Parallel(4) ## 병렬처리로 속도개선 하기 위함\n    mapper = joblib.delayed(load_row) ##얘랑 parallel이랑 이유는 모르겠는데 같이 씀 나중에 공부해볼것\n    \n    tasks = [mapper(row) for row in df.itertuples(False)]\n    res = pool(tqdm(tasks))\n    res = dict(res)\n    \n    return res","metadata":{"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# We cache the train set to reduce training time\n\naudio_image_store = load_data(df)","metadata":{"trusted":true},"execution_count":12,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/62874 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2851eaa804c64279964bf71c9ff2422f"}},"metadata":{}}]},{"cell_type":"code","source":"##dic으로 np array랑 filename이랑 매치돼 있는 녀석, 전체 파일이라 62874개있음\nlen(audio_image_store)","metadata":{"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"62874"},"metadata":{}}]},{"cell_type":"code","source":"## Training을 위한 dataset으로 만들기\nclass BirdClefDataset(Dataset):\n\n    def __init__(self, audio_image_store, meta, sr=SR, is_train=True, num_classes=NUM_CLASSES, duration=DURATION):\n        \n        self.audio_image_store = audio_image_store\n        self.meta = meta.copy().reset_index(drop=True)\n        self.sr = sr\n        self.is_train = is_train\n        self.num_classes = num_classes\n        self.duration = duration\n        self.audio_length = self.duration*self.sr\n    \n    @staticmethod\n    def normalize(image):\n        image = image.astype(\"float32\", copy=False) / 255.0\n        image = np.stack([image, image, image])\n        return image\n\n    def __len__(self):\n        return len(self.meta)\n    \n    def __getitem__(self, idx):\n        row = self.meta.iloc[idx]\n        image = self.audio_image_store[row.filename]\n\n        image = image[np.random.choice(len(image))]\n        image = self.normalize(image)\n        \n        \n        t = np.zeros(self.num_classes, dtype=np.float32) + 0.0025 # Label smoothing\n        t[row.label_id] = 0.995\n        \n        return image, t","metadata":{"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# ds 는 image랑 label smoothing 된\n\nds = BirdClefDataset(audio_image_store, meta=df, sr=SR, duration=DURATION, is_train=True)\nlen(ds)","metadata":{"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"62874"},"metadata":{}}]},{"cell_type":"code","source":"#ds[?] 로 랜덤으로 ?받아서 확인해봅시다.\n# x, y = ds[np.random.choice(len(ds))]\n\n# x는 RGB (128,281) numpy,   y는 label_Smooth 된 label\n# x.shape, y.shape, np.where(y >= 0.5)","metadata":{"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"##lbd.specshow(x[0])","metadata":{"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"markdown","source":"# Model 만들것!! ----------------------210429","metadata":{}},{"cell_type":"code","source":"def Convlayer(in_ch, out_ch, kernel_size=3, stride=2, use_leaky = True, use_inst_norm=True, use_pad=True):\n    \n    if use_pad:\n        conv = nn.Conv2d(in_ch, out_ch, kernel_size, stride, 1, bias=True)\n    else:\n        conv = nn.Conv2d(in_ch, out_ch, kernel_size, stride, 0, bias=True)\n     \n    \n    if use_inst_norm:\n        norm = nn.InstanceNorm2d(out_ch)\n    else:\n        norm = nn.BatchNorm2d(out_ch)\n        \n        \n    if use_leaky:\n        actv = nn.LeakyReLU(negative_slope=0.2, inplace=True)\n    else:\n        actv = nn.GELU()\n    \n    return nn.Sequential(conv, norm, actv)","metadata":{"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"class Resblock(nn.Module):\n    def __init__(self, in_features, use_dropout=True, dropout_ratio=0.5):\n        super().__init__()\n        models = list()\n        \n        models.append(Convlayer(in_features, in_features, 1, 1, use_leaky=True, use_inst_norm=False, use_pad=False))\n        models.append(nn.ReflectionPad2d(1))\n        models.append(Convlayer(in_features, in_features, 3, 1, use_leaky=True, use_inst_norm=False, use_pad=False))\n        models.append(nn.ReflectionPad2d(1))\n        models.append(Convlayer(in_features, in_features, 3, 1, use_leaky=True, use_inst_norm=False, use_pad=False))\n        \n        self.res = nn.Sequential(*models)\n        \n    \n    def forward(self, x):\n        x = x + self.res(x)\n        x = F.relu(x)\n        return x","metadata":{"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"def get_model(name, num_classes = NUM_CLASSES):\n    \n    '''\n    model = getattr(resnest_torch, name)(pretrained=True)\n    nb_ft = model.fc.in_features\n    print(nb_ft)\n    \n    model.fc = nn.Linear(nb_ft, num_classes)\n    '''\n    #xb = 100,3,128,281\n    #o = torch.Size([100, 397, 42, 93])\n    #yb = 100,397\n    \n    if 'resnet' in name: ##resnet 쓰고싶으면\n        model = torch.hub.load('pytorch/vision:v0.6.0', name, pretrained=True) ##python ver에 따라서 v0.6.0이나 0.9.0 등 고를것\n        model.fc = nn.Linear(model.fc.in_features, num_classes)\n\n    else: ##내가 만든 전용 모델 쓰고싶으면\n        model = list()\n        model.append(Resblock(3))\n        model.append(Convlayer(3, 1, 3, 3, use_leaky=False, use_inst_norm=False, use_pad=False))\n        model.append(nn.Flatten())\n        model.append(nn.Linear(3906,397)) ##3906\n        model = nn.Sequential(*model)\n    \n    \n    return model","metadata":{"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"def one_step(xb, yb, net, criterion, optimizer, scheduler = None):\n    torch.cuda.empty_cache()\n    \n    xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n    \n    \n    ## xb는 numpy, y는 label, net은 resnet등의 seqential\n    optimizer.zero_grad()\n    o = net(xb) #o는 자료 넣은 예측값\n    \n    \n    loss = criterion(o, yb)\n    loss.backward()\n    optimizer.step()\n    \n    with torch.no_grad():\n        l = loss.item()\n        \n        o = o.sigmoid() ##@@ 다른 활성화 쓸것\n        \n        yb = (yb > 0.5) * 1.0 ## threshold인듯\n        \n        ##sklearn에서 label smooth 쉽게하는 놈\n        lrap = label_ranking_average_precision_score(yb.cpu().numpy(), o.cpu().numpy())\n        \n        o = (o > 0.5) * 1.0\n        \n        prec = (o*yb).sum()/(1e-6 + o.sum())\n        rec = (o*yb).sum()/(1e-6 + yb.sum())\n        f1 = 2*prec*rec/(1e-6+prec+rec)\n        \n    if  scheduler is not None:\n        scheduler.step()\n    \n    \n    return l, lrap, f1.item(), rec.item(), prec.item()","metadata":{"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"@torch.no_grad()  ##함수 데코레이터 : evaluate를 torch.no_Grad인 상태로 실해하겠다.\ndef evaluate(net, criterion, val_loader):\n    \n    \n    net.eval()\n    \n    os, y = [], []\n    val_loader = tqdm(val_loader, leave=False, total = len(val_loader))\n    \n    for icount, (xb, yb) in enumerate(val_loader):\n        val_loader.set_description(\"in val_loader \")\n        torch.cuda.empty_cache()\n        \n        y.append(yb.to(DEVICE))\n        \n        xb = xb.to(DEVICE)\n        o = net(xb)\n        \n        os.append(o)\n    \n    # y, os를 list에서 torch로 변경\n    y = torch.cat(y)\n    o = torch.cat(os)\n    \n    l = criterion(o, y).item()\n    \n    o = o.sigmoid()\n    y = (y > 0.5)*1.0\n\n    lrap = label_ranking_average_precision_score(y.cpu().numpy(), o.cpu().numpy())\n\n    o = (o > 0.5)*1.0\n\n    prec = ((o*y).sum()/(1e-6 + o.sum())).item()\n    rec = ((o*y).sum()/(1e-6 + y.sum())).item()\n    f1 = 2*prec*rec/(1e-6+prec+rec)\n\n    return l, lrap, f1, rec, prec, ","metadata":{"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"def one_epoch(net, criterion, optimizer, scheduler, train_loader, val_loader):\n    net.train()\n    l, lrap, prec, rec, f1, icount = 0., 0., 0., 0., 0., 0 \n    train_loader = tqdm(train_loader, leave = False)\n    epoch_bar = train_loader\n    \n\n    for (xb, yb) in epoch_bar:\n        epoch_bar.set_description(\"for one step \")\n        \n        ##epoch 마다 step 한번씩 거치게 설계했음\n        _l, _lrap, _f1, _rec, _prec = one_step(xb, yb, net, criterion, optimizer)\n        \n        l += _l\n        lrap += _lrap\n        f1 += _f1\n        rec += _rec\n        prec += _prec\n        \n        icount += 1\n        \n        \n        if hasattr(epoch_bar, \"set_postfix\") and not icount%10:\n            epoch_bar.set_postfix(\n                loss=\"{:.6f}\".format(l/icount),\n                lrap=\"{:.3f}\".format(lrap/icount),\n                prec=\"{:.3f}\".format(prec/icount),\n                rec=\"{:.3f}\".format(rec/icount),\n                f1=\"{:.3f}\".format(f1/icount),\n            )\n    \n    scheduler.step()\n    \n    ##평균치 내서\n    l /= icount\n    lrap /= icount\n    f1 /= icount\n    rec /= icount\n    prec /= icount\n    \n    #eval에 넣어서 나온 값\n    l_val, lrap_val, f1_val, rec_val, prec_val = evaluate(net, criterion, val_loader)\n    \n    return (l, l_val), (lrap, lrap_val), (f1, f1_val), (rec, rec_val), (prec, prec_val)","metadata":{"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"##auto save 뺐음 나중에 제출할때 넣을것\nclass AutoSave:\n  def __init__(self, top_k=2, metric=\"f1\", mode=\"min\", root=None, name=\"ckpt\"):\n    self.top_k = top_k\n    self.logs = []\n    self.metric = metric\n    self.mode = mode\n    self.root = Path(root or MODEL_ROOT)\n    assert self.root.exists()\n    self.name = name\n\n    self.top_models = []\n    self.top_metrics = []\n\n  def log(self, model, metrics):\n    metric = metrics[self.metric]\n    rank = self.rank(metric)\n\n    self.top_metrics.insert(rank+1, metric)\n    if len(self.top_metrics) > self.top_k:\n      self.top_metrics.pop(0)\n\n    self.logs.append(metrics)\n    self.save(model, metric, rank, metrics[\"epoch\"])\n\n\n  def save(self, model, metric, rank, epoch):\n    t = time.strftime(\"%Y%m%d%H%M%S\")\n    name = \"{}_epoch_{:02d}_{}_{:.04f}_{}\".format(self.name, epoch, self.metric, metric, t)\n    name = re.sub(r\"[^\\w_-]\", \"\", name) + \".pth\"\n    path = self.root.joinpath(name)\n\n    old_model = None\n    self.top_models.insert(rank+1, name)\n    if len(self.top_models) > self.top_k:\n      old_model = self.root.joinpath(self.top_models[0])\n      self.top_models.pop(0)      \n\n    torch.save(model.state_dict(), path.as_posix())\n\n    if old_model is not None:\n      old_model.unlink()\n\n    self.to_json()\n\n\n  def rank(self, val):\n    r = -1\n    for top_val in self.top_metrics:\n      if val <= top_val:\n        return r\n      r += 1\n\n    return r\n  \n  def to_json(self):\n    # t = time.strftime(\"%Y%m%d%H%M%S\")\n    name = \"{}_logs\".format(self.name)\n    name = re.sub(r\"[^\\w_-]\", \"\", name) + \".json\"\n    path = self.root.joinpath(name)\n\n    with path.open(\"w\") as f:\n      json.dump(self.logs, f, indent=2)","metadata":{"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"##한 fold는 나눠진 dataset당 epoch 다 돌리는거의미\n\ndef one_fold(model_name, fold, train_set, val_set, epochs=20, save=True, save_root=None):\n    \n    save_root = Path(save_root) or MODEL_ROOT\n    saver = AutoSave(root=save_root, name=f\"birdclef_{model_name}_fold{fold}\", metric=\"f1_val\")\n    \n    \n    net = get_model(model_name).to(DEVICE)\n    criterion = nn.BCEWithLogitsLoss()\n    optimizer = optim.Adam(net.parameters(), lr=8e-4)\n    \n    #lr 스케쥴러 -> Set the learning rate of each parameter group using a cosine annealing schedule\n    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, eta_min=1e-5, T_max=epochs)\n    \n    \n    \n    ###여기서부터 data 불러오는 과정\n    #Train Mel_img를 Dataset으로 불러옴\n    train_data = BirdClefDataset(audio_image_store, meta=df.iloc[train_set].reset_index(drop=True),\n                           sr=SR, duration=DURATION, is_train=True)\n    \n    #Train dataloader 예시 (batchsize, 3, 256, 256) 처럼 불러오게 됨\n    train_loader = DataLoader(train_data, batch_size=TRAIN_BATCH_SIZE, num_workers=TRAIN_NUM_WORKERS, shuffle=True, pin_memory=True)\n    \n    \n    ##Val\n    val_data = BirdClefDataset(audio_image_store, meta=df.iloc[val_set].reset_index(drop=True),  sr=SR, duration=DURATION, is_train=False)\n    val_loader = DataLoader(val_data, batch_size=VAL_BATCH_SIZE, num_workers=VAL_NUM_WORKERS, shuffle=False, pin_memory=True)\n    \n    \n    ####진행####\n    epochs_bar = tqdm(list(range(epochs)), leave=False)\n    for epoch in epochs_bar:\n        epochs_bar.set_description(f\"--> [EPOCH {epoch:02d}]\")\n        net.train()\n        \n        \n        ##한 epoch 돌려서 return합시다\n        (l, l_val), (lrap, lrap_val), (f1, f1_val), (rec, rec_val), (prec, prec_val) \\\n        = one_epoch(net=net,\n                    criterion=criterion,\n                    optimizer=optimizer,\n                    scheduler=scheduler,\n                    train_loader=train_loader,\n                    val_loader=val_loader,\n                   )\n        \n        \n        epochs_bar.set_postfix(\n            loss=\"({:.6f}, {:.6f})\".format(l, l_val),\n            prec=\"({:.3f}, {:.3f})\".format(prec, prec_val),\n            rec=\"({:.3f}, {:.3f})\".format(rec, rec_val),\n            f1=\"({:.3f}, {:.3f})\".format(f1, f1_val),\n            lrap=\"({:.3f}, {:.3f})\".format(lrap, lrap_val),\n        )\n        \n        \n        print(\n            \"[{epoch:02d}] loss: {loss} lrap: {lrap} f1: {f1} rec: {rec} prec: {prec}\".format(\n                epoch=epoch,\n                loss=\"({:.6f}, {:.6f})\".format(l, l_val),\n                prec=\"({:.3f}, {:.3f})\".format(prec, prec_val),\n                rec=\"({:.3f}, {:.3f})\".format(rec, rec_val),\n                f1=\"({:.3f}, {:.3f})\".format(f1, f1_val),\n                lrap=\"({:.3f}, {:.3f})\".format(lrap, lrap_val)\n            )\n        )\n        if save:\n            metrics = {\n                \"loss\": l, \"lrap\": lrap, \"f1\": f1, \"rec\": rec, \"prec\": prec,\n                \"loss_val\": l_val, \"lrap_val\": lrap_val, \"f1_val\": f1_val, \"rec_val\": rec_val, \"prec_val\": prec_val,\n                \"epoch\": epoch,\n            }\n            \n            saver.log(net, metrics)","metadata":{"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"def train(model_name, epochs = 10, save= True, n_splits=5, seed=177, save_root=None, suffix=\"\", folds=None):\n    gc.collect()\n    torch.cuda.empty_cache()\n    \n    ##Save 관련...\n    save_root = save_root or MODEL_ROOT/f\"{model_name}{suffix}\"\n    save_root.mkdir(exist_ok=True, parents=True)\n    \n    fold_bar = tqdm(df.reset_index().groupby(\"fold\").index.apply(list).items(), total=df.fold.max()+1)\n    \n    for fold, val_set in fold_bar:\n        if folds and not fold in folds:\n            continue\n            \n        print(f\"\\n############################### [FOLD {fold}]\")\n        fold_bar.set_description(f\"[FOLD {fold}]\")\n        train_set = np.setdiff1d(df.index, val_set)\n        \n        one_fold(model_name, fold=fold, train_set=train_set , val_set=val_set , epochs=epochs, save=save, save_root=save_root)\n        \n        gc.collect()\n        torch.cuda.empty_cache()","metadata":{"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"train(\"resnet152\", epochs=4, suffix=f\"_sr{SR}_d{DURATION}_v1_v1\", folds=[0])","metadata":{"trusted":true},"execution_count":28,"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/5 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cc6432569adc4eedaf9593eeb053759b"}},"metadata":{}},{"name":"stdout","text":"\n############################### [FOLD 0]\n","output_type":"stream"},{"name":"stderr","text":"Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.6.0\nDownloading: \"https://download.pytorch.org/models/resnet152-b121ed2d.pth\" to /root/.cache/torch/hub/checkpoints/resnet152-b121ed2d.pth\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0.00/230M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aa8a6c0885d14138a6c3975cae109b3a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1006 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/252 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"[00] loss: (0.031898, 0.029382) lrap: (0.103, 0.201) f1: (0.003, 0.029) rec: (0.002, 0.015) prec: (0.062, 0.551)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1006 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/252 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"[01] loss: (0.028110, 0.027806) lrap: (0.323, 0.360) f1: (0.074, 0.140) rec: (0.040, 0.079) prec: (0.620, 0.632)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1006 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/252 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"[02] loss: (0.026075, 0.025941) lrap: (0.505, 0.515) f1: (0.251, 0.304) rec: (0.151, 0.189) prec: (0.824, 0.779)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1006 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/252 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"[03] loss: (0.024536, 0.024447) lrap: (0.628, 0.633) f1: (0.415, 0.473) rec: (0.274, 0.326) prec: (0.888, 0.860)\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TEST_AUDIO_ROOT = Path(\"../input/birdclef-2021/test_soundscapes\")\nSAMPLE_SUB_PATH = \"../input/birdclef-2021/sample_submission.csv\"\nTARGET_PATH = None\n    \nif not len(list(TEST_AUDIO_ROOT.glob(\"*.ogg\"))):\n    TEST_AUDIO_ROOT = Path(\"../input/birdclef-2021/train_soundscapes\")\n    SAMPLE_SUB_PATH = None\n    # SAMPLE_SUB_PATH = \"../input/birdclef-2021/sample_submission.csv\"\n    TARGET_PATH = Path(\"../input/birdclef-2021/train_soundscape_labels.csv\")","metadata":{"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"data = pd.DataFrame(\n    [(path.stem, *path.stem.split(\"_\"), path) for path in Path(TEST_AUDIO_ROOT).glob(\"*.ogg\")],\ncolumns = ['filename', 'id', 'site', 'date', 'filepath'])\n\nprint(data.shape)","metadata":{"trusted":true},"execution_count":30,"outputs":[{"name":"stdout","text":"(20, 5)\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##\n'''\nThanks for the decent notebook, it really helped me a lot! :)\nHere is my question.\nAs far as I understand, 6 images were generated when one audio file is about 30 seconds long.\nAnd in BirdClefDataset, you chose only one random image from these 6 images of one audio file and used it to train model. Is that correct?\nIf it is correct, then is there any reasons for using only one random image? not all images?\n\nI'm a beginner and I might not fully understand your code.\nPlease point me out if I was wrong :)\n'''","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"##일단 체크포인트 ##@@로 해놨음","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"회자 식별 관련 연구같은거 보면 좋음\n\n1. https://academy.allaboutbirds.org/the-language-of-birds/\n-> 주파수 별로 쪼개서(고주파, 저주파, 전) learning 시킬것\n-> 진폭과 주파수를 다 고려할것\nhttps://www.youtube.com/watch?v=2FMeJdYh5Sc\n-> a) spectrogram -> CNN /2차원\n   b) without feature extraction frame level 로 -> 1d cnn\n   a,b 를 score level ensemble\n    \n    \n2. 사람의 음성이 아니기때문에 mel 차원으로 할 필요 없을듯\n\n2. 아니면 차원수를 늘려서 RGB처럼 n차원으로(고주파, 저주파, 전체) 분석하기\nhttps://www.youtube.com/watch?v=g0pvHkq-BIA\n    \n    \n3.음성신호의 noise 제거하기\n ->VAD(voice activity detection) 으로 무음구간의 프레임 음향특성제거\n ->고민을 좀 해봐야 할듯\n\n4.분류 모델이기 때문에 다양한 손실함수 사용하고 비교해볼것\n ->labelsmoothing\n    \n5.Concrete Drop out 등 drop out technique 사용해보기\n6.Sound Mixup tech 사용해보기 -> 메모장에 있음\n7.TS learning 활용\nhttps://www.youtube.com/watch?v=2FMeJdYh5Sc\n    \n8.활성화 함수(GELU, Leaky중에서)\n9. model에 residual block 사용하기\n10. pooling 은 overfitting 에 유리하다, padding은 정보압축해서 속도면에서 유리하다\n11. optimizer 은 SGD, AMSGrad 등등 고려","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"'''data loader 내부 확인\ndataiter = iter(데이터로더 이름)\nimages, labels = dataiter.next()\nprint(images.shape, labels.shape)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}